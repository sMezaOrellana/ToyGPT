{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17be996a-71ef-41c8-825a-e8a8a18ed706",
   "metadata": {},
   "source": [
    "# Building a NanoGPT from Scratch: Attempting to Understand the Transformer Architecture\n",
    "## 0. Introduction\n",
    "My journey began in late 2024/early 2025 when I discovered Andrej Karpathy's video **Let's build GPT: from scratch, in code, spelled out**. The two-hour deep dive provided a clear and, low-level overview of the architecture. While I grasped the general framework of the Transformer, the inner workings of the Attention mechanism remained opaque.\n",
    "\n",
    "The challenge wasn't just the architecture itself; it was my **foundation**. My memories of undergraduate linear algebra, & machine learning were nonexistent, as if by now those tokens were outside of my context window. necessitating a serious refresh on core concepts like **matrix multiplication, dot product, loss function, backpropagation, and tensor dimensionality**.\n",
    "\n",
    "Throughout the year, I repeatedly returned to Karpathyâ€™s explanation. **Each time, I peeled back a new layer of complexity**, steadily improving my grasp of the Transformer's operation. What started as a vague concept slowly transformed (hehe) into a detailed understanding.\n",
    "\n",
    "This document is the culmination of that effort my attempt to synthesize everything Iâ€™ve learned about how these **\"thinking machines\"** work. I aimed for clarity and depth, and I hope it proves genuinely useful to others starting this same path.\n",
    "\n",
    "This notebook provides a technical overview and guide for a minimal implementation of a Generative Pre-trained Transformer (**NanoGPT**), built using PyTorch. This project serves as a clear, educational reference for understanding the core components of modern large language models, from data preparation to the self-attention mechanism and text generation.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Data Acquisition and Character-Level Tokenization\n",
    "\n",
    "The first step in any language modeling project is preparing the text data. For this notebook, we used a public domain work of literature: **Don Quijote de la Mancha**.\n",
    "\n",
    "### Data Loading and Tokenization\n",
    "The text was downloaded directly from Project Gutenberg using `curl`. A simple **character-level tokenizer** was implemented by mapping every unique character to an integer ID.\n",
    "\n",
    "* The **vocabulary size** ($\\boldsymbol{V}$ or `vocab_size`) is determined by the number of unique characters.\n",
    "* The entire corpus is encoded into a long list of integers.\n",
    "\n",
    "### Data Preparation and Dimensions\n",
    "The encoded corpus is split into training and validation sets (90/10). For training, we sample batches of input sequences ($X$) and target sequences ($Y$).\n",
    "\n",
    "Let $B$ be the batch size (which is implicitly set to `n_embd` in the `sample_data` call for the loss estimate) and $T$ be the context length (`block_size`).\n",
    "\n",
    "* **Input Data:** The input batch $X$ is a tensor of shape $(B, T)$.\n",
    "* **Target Data:** The target batch $Y$ is also a tensor of shape $(B, T)$, where $Y_{i, t}$ is the correct prediction for the token $X_{i, t+1}$.\n",
    "\n",
    "### Sources\n",
    "\n",
    "* **Attention Is All You Need**\n",
    "* **LLM Visualization**\n",
    "* **Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch**\n",
    "* **Andrej Karpathy's makemore/nanoGPT Video**\n",
    "* **Language Models are Unsupervised Multitask Learners (GPT-2 Paper)**\n",
    "* **Wikipedia: Byte-pair encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a2c76-aa67-4690-9b7c-e7267f1bc927",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://www.gutenberg.org/files/2000/2000-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e0fa6-016a-4e68-9e73-c2863b37527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd380d93-6a6f-418e-be13-33a9ebf29631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "# Check for CUDA availability and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "05fa9757-8385-4ed0-bd76-2a461a7bac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./2000-0.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8', newline='\\n') as f:\n",
    "    contents = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "656e554b-7c65-453f-8bb7-b3d14bb2312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nde se prosigue la narraciÃ³n de la desgracia de\n",
      "nuestro caballero\n",
      "\n",
      "Del donoso y grande escrutinio que el cura y el\n",
      "barbero hicieron en la librerÃ­a de nuestro ingenioso hidalgo\n",
      "\n",
      "De la segunda salida de nuestro buen caballero don\n",
      "Quijote de la Mancha\n",
      "\n",
      "Del buen suceso que el valeroso don Quijote tuvo en\n",
      "la espantable y jamÃ¡s imaginada aventura de los molinos de viento, con\n",
      "otros sucesos dignos de felice recordaciÃ³n\n",
      "\n",
      "Donde se concluye y da fin a la estupenda batalla que\n",
      "el gallardo viz\n"
     ]
    }
   ],
   "source": [
    "num_tokens = 500\n",
    "offset = 3\n",
    "print(contents[num_tokens*offset:num_tokens*offset+num_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e98ede-0e4a-4e4f-af69-d0faab15e3bd",
   "metadata": {},
   "source": [
    "### Character-Level Tokenization and Vocab Creation\n",
    "\n",
    "We first create a simple **character-level tokenizer**. This approach is simple and highly robust (never fails on an OOV word) but is computationally inefficient for large models due to sequence bloat.\n",
    "\n",
    "1.  **Extract Vocabulary:** Find all unique characters in the corpus.\n",
    "2.  **Mapping:** Create lookup tables (`stoi` for string-to-integer and `itos` for integer-to-string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "64fb421c-0302-4f1a-8625-1353d0d02ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "chars = sorted(list(set(contents)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "b0ca1749-72ca-4d5e-af89-5db1a9ca71c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 58, 71, 60, 65, 72, 2, 45, 58, 71, 83, 58, 2]\n",
      "Sancho Panza \n"
     ]
    }
   ],
   "source": [
    "a = \"Sancho Panza \"\n",
    "print(encode(a))\n",
    "print(decode(encode(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebeea29-bab1-49b2-a663-51dd6510e559",
   "metadata": {},
   "source": [
    "### Batching Function\n",
    "\n",
    "The **Transformer** operates on fixed-length chunks. We define a function to sample random batches of input ($X$) and target ($Y$) sequences.\n",
    "\n",
    "* **Context Length ($\\boldsymbol{T}$):** `block_size` (e.g., 32) is the maximum sequence length the model processes at once.\n",
    "* **Batch Size ($\\boldsymbol{B}$):** The number of independent sequences processed in parallel.\n",
    "\n",
    "The target $\\boldsymbol{Y}$ is simply the input $\\boldsymbol{X}$ shifted one position to the right, as the model predicts the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e19bd19-b115-4401-8a22-f1172f7abe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(training_data: str, split_ratio: float = 0.9) -> tuple[str,str]:\n",
    "    training_data_length = len(training_data)\n",
    "    marker = int(split_ratio * training_data_length)\n",
    "    return training_data[:marker], training_data[marker:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57d0de4e-e662-40e3-9b70-a28b53ea36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def sample_data(data, len_samples:int=16, num_samples:int=8) -> torch.Tensor:\n",
    "    len_data = len(data)\n",
    "    random_indices = torch.randint(0, len_data - len_samples, (num_samples,))\n",
    "    sample_x = torch.stack([\n",
    "        torch.tensor(data[r_ind:r_ind+len_samples]) \n",
    "        for r_ind in random_indices\n",
    "    ])\n",
    "    sample_y = torch.stack([\n",
    "        torch.tensor(data[r_ind+1:r_ind+1+len_samples]) \n",
    "        for r_ind in random_indices\n",
    "    ])\n",
    "    return sample_x, sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd0520d4-7554-4e8e-8118-3e273b0c4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_contents = encode(contents)\n",
    "training_data, validation_data = split_data(encoded_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3895a97-46e8-43d7-9e9f-b15c602a5234",
   "metadata": {},
   "source": [
    "## 2. The Transformer Block and Attention Mechanism\n",
    "\n",
    "The core of the NanoGPT is the **Transformer Block**, based on the seminal \"Attention Is All You Need\" paper. This is a decoder-only architecture, characteristic of GPT models.\n",
    "\n",
    "### The NanoGPT Architecture\n",
    "The model uses several learned components to transform the input:\n",
    "1.  **Token Embeddings:** Maps the input IDs $(B, T)$ to a continuous space of size $C$ (`embedding_dim = 2048`).\n",
    "    $$\\text{Token Embeddings}: (B, T) \\rightarrow (B, T, C)$$\n",
    "2.  **Positional Embeddings:** A learned encoding added to the token embeddings to provide sequential context, as attention itself is order-agnostic.\n",
    "3.  **Final Input Vector:** The sum of the token and positional embeddings forms the final input vector to the blocks.\n",
    "\n",
    "### Causal Self-Attention and Matrix Sizes\n",
    "The `AttentionHead` implements the scaled dot-product attention, modified for causality (GPT-style).\n",
    "\n",
    "Let $H$ be the size of a single attention head (`head_size`). $H = C / N_h$, where $N_h$ is the number of heads (`num_heads = 12`).\n",
    "\n",
    "1.  **Linear Projections (Q, K, V):** The input vector $x$ (shape $(B, T, C)$) is projected into Query ($\\boldsymbol{Q}$), Key ($\\boldsymbol{K}$), and Value ($\\boldsymbol{V}$) matrices.\n",
    "    $$\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V} = W_Q(x), W_K(x), W_V(x) \\rightarrow (B, T, H)$$\n",
    "2.  **Attention Logits ($\\Omega$):** The raw attention scores ($\\Omega$) are calculated by the dot product of $Q$ and $K^T$. This score determines how much token $i$ should pay attention to token $j$.\n",
    "    $$\\Omega = \\boldsymbol{Q}\\boldsymbol{K}^T \\rightarrow (B, T, H) \\times (B, H, T) \\rightarrow (B, T, T)$$\n",
    "3.  **Scaling and Masking:** $\\Omega$ is scaled by $\\frac{1}{\\sqrt{H}}$ to stabilize gradients. A lower-triangular mask (`tril`) is applied, setting entries corresponding to future tokens to $-\\infty$. This is the **causal mask**, enforcing that the prediction for token $t$ can only depend on tokens $1$ through $t$.\n",
    "4.  **Attention Weights ($W$):** The masked logits are passed through a softmax function, resulting in the attention weights $W$. These weights are the probability distribution over past tokens for the current token.\n",
    "    $$W = \\text{softmax}(\\Omega_{\\text{masked}} / \\sqrt{H}) \\rightarrow (B, T, T)$$\n",
    "5.  **Context Vector (Output):** The weights $W$ are multiplied by the Value matrix $V$ to create the context vector, which is the output of the attention head.\n",
    "    $$\\text{Output} = W\\boldsymbol{V} \\rightarrow (B, T, T) \\times (B, T, H) \\rightarrow (B, T, H)$$\n",
    "\n",
    "The **Multi-Head Attention** module runs $N_h$ heads in parallel, concatenates their outputs (shape $(B, T, N_h \\cdot H)$), and projects the result back to $C$ using a final linear layer.\n",
    "\n",
    "The **TransformerBlock** wraps this, applying Layer Normalization (Post-LN style), as originally implemented in **Attention is all you need** and **residual connections**: $x' = x + \\text{LayerNorm}(\\text{MultiHead}(x))$. This structure ensures gradients can flow easily through the network.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "296f304d-8f1d-43ca-abf4-9344652c45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, n_embd:int, head_size:int, block_size:int, dropout:int=0):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.W_query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.W_key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.W_value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B,T,C = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        omega = queries @ keys.mT # * C**-0.5\n",
    "\n",
    "        omega_masked = omega.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        attention_weights = nn.functional.softmax(omega_masked / self.head_size**0.5, dim=-1)\n",
    "        wei = self.dropout(attention_weights)\n",
    "\n",
    "        context_vector = wei @ values # (num_samples, num_words, num_embeddings_per_word)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self, num_heads:int, n_embd:int, head_size:int, block_size:int, dropout:int=0.0):\n",
    "        super().__init__()\n",
    "        self.attention_blocks = nn.ModuleList([AttentionHead(n_embd, head_size, block_size) for _ in range(num_heads)])\n",
    "        self.linear_projection = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        attention_weights = torch.cat([head(x) for head in self.attention_blocks], dim=-1)\n",
    "        lin = self.linear_projection(attention_weights)\n",
    "        \n",
    "        return self.dropout(lin)\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, n_embd:int, block_size:int, dropout:int=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.Linear(n_embd, 4 * n_embd)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ln2 = nn.Linear(4 * n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.ln1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.ln2(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads:int, n_embd:int, head_size:int, block_size:int):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiAttentionHead(num_heads, n_embd, head_size, block_size)\n",
    "        self.ffn = FeedForwardNetwork(n_embd, block_size)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #x = x + self.multi_head_attention(self.ln1(x))\n",
    "        #x = x + self.ffn(self.ln2(x))\n",
    "\n",
    "        x = x + self.ln1(self.multi_head_attention(x))\n",
    "        x = x + self.ln2(self.ffn(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "274e8d37-ae86-4717-bc08-d079db553c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very simple and stupid GPT model!\n",
    "class NanoGPT(nn.Module):\n",
    "    def __init__(self, num_layers:int, vocab_size:int, embedding_dim:int, num_heads:int, block_size:int):\n",
    "        super().__init__()\n",
    "        head_size = embedding_dim // num_heads\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.positional_embeddings = nn.Embedding(block_size,embedding_dim)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(num_heads, embedding_dim, head_size, block_size) for _ in range(num_layers)])\n",
    "    \n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, targets: torch.Tensor|None=None) -> torch.Tensor:\n",
    "        B,T = input_ids.shape\n",
    "        word_embeddings = self.embedding(input_ids)\n",
    "        pos = torch.arange(T)\n",
    "        pos = pos.to(device)\n",
    "        \n",
    "        positional_embeddings = self.positional_embeddings(pos)\n",
    "\n",
    "        final_vector = word_embeddings + positional_embeddings\n",
    "\n",
    "        x = self.blocks(final_vector) # (B,T,C)\n",
    "        x = self.ln1(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        num_samples, num_words, num_embeddings = logits.shape\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(num_samples * num_words, num_embeddings)\n",
    "            targets = targets.view(num_samples * num_words)\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = nn.functional.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4820f12f-a245-479d-88f7-6e54cd339743",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_data, val_data, block_size, n_embd, eval_iters: int = 20):\n",
    "    # Dictionaries to store the average loss for each split\n",
    "    out = {}\n",
    "    model.eval() # Set model to evaluation mode (crucial for dropout/BatchNorm)\n",
    "\n",
    "    for split_name, data in [('train', train_data), ('val', val_data)]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            # Sample data from the current split (train or val)\n",
    "            xb, yb = sample_data(data, block_size, n_embd)\n",
    "            \n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            # Forward pass to get logits and loss\n",
    "            logits, loss = model(xb, yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[split_name] = losses.mean()\n",
    "\n",
    "    model.train() # Set model back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4fab10-f77d-471d-82c1-becdbc3bb952",
   "metadata": {},
   "source": [
    "## 3. Training Optimization: Loss and Gradient Descent\n",
    "\n",
    "### Loss Function\n",
    "The model's final layer, the language model head (`lm_head`), produces **logits** of shape $(B, T, V)$. These logits represent the raw, unnormalized scores for every possible next character in the vocabulary at every time step.\n",
    "\n",
    "The model uses the standard **Cross-Entropy Loss** for classification tasks, comparing the predicted logits against the true target tokens $Y$.\n",
    "\n",
    "$$\\text{Loss} = \\text{CrossEntropyLoss}(\\text{Logits}, \\text{Targets})$$\n",
    "\n",
    "To calculate this, the logits and targets are often reshaped into two-dimensional vectors, effectively treating all $B \\times T$ predictions in the batch as independent classification problems:\n",
    "* $\\text{Logits}: (B \\cdot T, V)$\n",
    "* $\\text{Targets}: (B \\cdot T)$\n",
    "\n",
    "### Gradient Descent and Optimization\n",
    "The goal of training is to find the model parameters (weights) that minimize this loss. This is achieved using **Gradient Descent**, specifically the **AdamW optimizer**.\n",
    "\n",
    "1.  **Forward Pass:** Calculate the loss.\n",
    "2.  **Backward Pass (Backpropagation):** The `loss.backward()` call computes the gradient (derivative) of the loss with respect to every single parameter in the model.\n",
    "3.  **Parameter Update:** The `optimizer.step()` function uses these calculated gradients to update the model's parameters in the direction that decreases the loss, aiming for convergence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b1581ec-be1a-4de1-84fc-bed04746a629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.110522 M parameters\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4\n",
    "n_embd = 248\n",
    "num_heads = 12\n",
    "block_size = 2048 >> 2\n",
    "gpt = NanoGPT(num_layers, vocab_size, n_embd, num_heads, block_size)\n",
    "gpt.to(device)\n",
    "print(sum(p.numel() for p in gpt.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15e4923b-8aa0-4e56-982b-c2908b21d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.827203750610352\n",
      "2.2913737297058105\n",
      "2.2512147426605225\n",
      "2.1931519508361816\n",
      "2.0742852687835693\n",
      "1.6983325481414795\n",
      "1.5868492126464844\n",
      "1.4134609699249268\n",
      "1.313541054725647\n",
      "1.2511333227157593\n",
      "1.2177703380584717\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3)\n",
    "total_steps = 1000  \n",
    "for steps in range(total_steps): # increase number of steps for good results...\n",
    "    # sample a batch of data\n",
    "\n",
    "    xb, yb = sample_data(training_data, block_size, n_embd)\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    # evaluate the loss\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if steps % (total_steps//10) == 0 or steps ==  total_steps - 1:\n",
    "        print(loss.item())\n",
    "    del xb, yb, logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa5b274f-6e12-4776-a15c-d2e7046caac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]], device='cuda:0')\n",
      "tensor([[46, 78, 66, 62, 71,  2, 62, 75, 62, 76, 28]], device='cuda:0')\n",
      "Quien eres?\n",
      "Ella despa estÃ¡ ser riqueza la gente que se mucho en venturas,\n",
      "asÃ­, dejando que ya a gobernador, y se aproprese complasen sangre si\n",
      "estavores todos los ojores, y su alcaÃºn necesica sobre vuestra merced\n",
      "dÃ¡dis por quien no lo viniese yo que debe en de todas vestidas suerte;\n",
      "los casaden mÃ¡s se salamente, y es especiallos no regalÃ³ a vino alguna que le\n",
      "habÃ­an truinos como jurarona, y en cuanto de peligro premoza el de Caballeroâ€”\n",
      "sirve lo que se me haga malicia; mas, no le pude todo o a cabo a don\n",
      "llorcabar, como si agude); la cual avella y que precios Ã¡goles, tanto esto\n",
      "particulo para conocerle habÃ­a, que yo la bogarÃ¡ en la mance, no por deseÃ¡\n",
      "sentimiento mil de Cardenio, mas cuitan, y que sean anastase,\n",
      "tÃºnomplejo,\n",
      "que yo puesto es cosas fortales de GriÃ³neril\n",
      "\n",
      "Dulcincia. Todavio te tomamos in paso Sancho Â¡dicieron esas\n",
      "Basilio por persona y justiciÃ³n tambiÃ©n los hizo\n",
      "de fin lamarme;\n",
      "\n",
      "maneele de los venÃ­an cuentro, con haciÃ©ndoo por ello, inclindus que todo\n",
      "los cortesid\n"
     ]
    }
   ],
   "source": [
    "starting_idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(starting_idx)\n",
    "starting_idx = torch.tensor([encode(\"Quien eres?\")], dtype=torch.long, device=device)\n",
    "print(starting_idx)\n",
    "print(decode(gpt.generate(idx = starting_idx, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a3875-cd76-4122-8a79-b4e5bd094011",
   "metadata": {},
   "source": [
    "## 4. Why Character-Level Tokenization is Inefficient\n",
    "\n",
    "The character-level approach, while simple and robust against out-of-vocabulary (OOV) words, is highly **inefficient** for training and running modern Transformer-based LLMs:\n",
    "\n",
    "1.  **Excessive Sequence Length (Sequence Bloat):** A single common word requires multiple tokens (characters). This maximizes the sequence length ($T$) for any given text. Texts encoded at the character level are often **3 to 5 times longer** than those encoded using subword methods.\n",
    "2.  **Quadratic Computational Cost:** The self-attention mechanism, the core operation of the Transformer, has a computational complexity that is **quadratic** with respect to the sequence length, $O(T^2)$. If the sequence length is four times longer, the computation for attention is $\\sim 16$ times slower.\n",
    "3.  **Reduced Effective Context Window:** Since LLMs have a fixed maximum context length, excessive tokenization limits the number of actual words the model can \"see\" at once, hindering its ability to capture **long-range dependencies** necessary for high-quality language understanding and generation.\n",
    "4.  **Low Information Density:** Individual character tokens carry very **low semantic meaning**. The model must dedicate substantial capacity to learning the basic compositional structure of words rather than focusing on high-level linguistic relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Next Steps: Implementing Byte Pair Encoding (BPE)\n",
    "\n",
    "To overcome the inefficiencies of character-level tokenization, an implementation of the **Byte Pair Encoding (BPE)** algorithm was written as the next step. BPE, as used in models like GPT-2, balances character-level flexibility with word-level efficiency.\n",
    "\n",
    "The BPE implementation in the notebook followed the general algorithm described on **Wikipedia**:\n",
    "1.  **Iterative Merge:** Start with the input string represented by bytes.\n",
    "2.  **Substitution:** Recursively identify the most frequent adjacent pair of tokens and replace them with a new, single token ID (starting from value 256).\n",
    "3.  **Dictionary:** The substitution is recorded in a `dictionary` mapping the new token ID to the two tokens it replaces, allowing for decoding.\n",
    "\n",
    "### Note on GPT-2 Paper and BPE\n",
    "The implemented BPE was a pure algorithmic approach and **did not** incorporate the specific byte-level exception mentioned in the GPT-2 paper. This exception is typically added to prevent the merging of common words with preceding or trailing punctuation (e.g., separating `dog` from `.`, `!`, or `?`) to preserve valuable vocabulary slots and improve compression efficiency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a6aeea-8361-4064-9c04-b47a0da66e8a",
   "metadata": {},
   "source": [
    "## 5.1 Byte Pair Encoding Training\n",
    "\n",
    "The **BPE Encoder** section of the notebook implements a **subword tokenization** algorithm. This is a crucial step in language models like GPT, as it balances the need for a small, manageable vocabulary size with the ability to represent rare or unseen words (the \"out-of-vocabulary\" or OOV problem). \n",
    "\n",
    "### The Role of BPE\n",
    "\n",
    "* **Balancing Vocabulary and Sequence Length:**\n",
    "    * Using every single word as a token results in a huge, sparse vocabulary, and many words are rarely seen.\n",
    "    * Using individual characters results in a tiny vocabulary, but the input **sequence length** becomes very long, increasing the computational cost of the Transformer's attention mechanism.\n",
    "* **The BPE Solution:** BPE tokenizes text into **subwords** (e.g., `_pre` + `vent` + `ing`), where the vocabulary consists of common characters, character pairs, and subwords. This keeps the final vocabulary size reasonable while keeping the average input sequence length shorter than character-level tokenization.\n",
    "\n",
    "### The \"Training\" Process\n",
    "\n",
    "The BPE training process is an iterative, greedy algorithm that learns the most efficient way to combine base units (initial characters) into larger, frequent tokens.\n",
    "\n",
    "1.  **Initialization:** The process begins with a base vocabulary containing every unique **character** found in the entire training text.\n",
    "2.  **Frequency Analysis:** The training loop repeatedly scans the entire corpus to find the **most frequent adjacent pair** of tokens (e.g., `('t', 'h')`).\n",
    "3.  **Merge Rule Creation:** Once the most frequent pair is identified, a **merge rule** is created, and the two tokens are replaced everywhere in the corpus with a new, single token (e.g., `('t', 'h')` is merged into `'th'`). This new token is added to the vocabulary.\n",
    "4.  **Iteration:** This merging process is repeated for a specified number of merge operations (e.g., 50,000 steps) or until a target vocabulary size is reached.\n",
    "5.  **The `merge_dict`:** The final ordered set of merge rules (the `merge_dict` you see in the notebook) is the entire learned **BPE encoder**. This dictionary is what is used by the encoding and decoding functions to process new, unseen text.\n",
    "\n",
    "### Examples of LLM Vocabulary Sizes\n",
    "\n",
    "The final size of the BPE vocabulary is determined by the number of merge operations performed on the base character set, striking a balance between compression efficiency and vocabulary size (the size of the embedding matrix).\n",
    "\n",
    "| Model Family | Vocabulary Size | Tokenizer Base |\n",
    "| :--- | :--- | :--- |\n",
    "| **GPT-2** | **50,257** | Byte-level BPE |\n",
    "| **GPT-3 / GPT-3.5** | **~50,000 - 100,256** | Byte-level BPE (OpenAI has since standardized on a larger, more efficient vocabulary, often **100,256**, for modern models like GPT-4, referred to as `cl100k_base`). |\n",
    "| **Llama 1 / Llama 2** | **32,000** | SentencePiece (BPE variant) |\n",
    "| **Llama 3** | **128,000** | Tiktoken-based BPE (a significant increase for multilingual support) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d75bebeb-f524-4949-873b-ad1b5ae71621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_encoding(input_bytes:list[bytes], dictionary={}, unused=256, level=0, max_level=20, vocab_size=50000) -> list[int]:\n",
    "    while level < max_level or unused != vocab_size:\n",
    "        len_bytes = len(input_bytes)\n",
    "        byte_pair_bin = {}\n",
    "    \n",
    "        # here walk through the input and calculate the byte pairs\n",
    "        for i in range(len_bytes-1):\n",
    "            packed_bp = input_bytes[i] | (input_bytes[i+1] << 32)\n",
    "    \n",
    "            if packed_bp not in byte_pair_bin:\n",
    "                byte_pair_bin[packed_bp] = 1\n",
    "            else:\n",
    "                byte_pair_bin[packed_bp] += 1\n",
    "    \n",
    "        # here we walk through the input in order to replace the byte pair\n",
    "        index = 0\n",
    "        top_bp = max(byte_pair_bin, key=byte_pair_bin.get)\n",
    "        v_size = max(byte_pair_bin)\n",
    "    \n",
    "        if byte_pair_bin[top_bp] == 1 :\n",
    "            break\n",
    "        \n",
    "        encoded = []\n",
    "        update_unused = False\n",
    "        while index < len_bytes:\n",
    "            if index == len_bytes - 1:\n",
    "                encoded.append(input_bytes[index])\n",
    "                break\n",
    "                \n",
    "            # why pack the bp? why not use a tuple? because we want to go fast\n",
    "            packed_bp = input_bytes[index] | (input_bytes[index+1] << 32)\n",
    "    \n",
    "            if packed_bp == top_bp:\n",
    "                encoded.append(unused)\n",
    "                dictionary[unused] = top_bp\n",
    "                index += 2\n",
    "                update_unused = True\n",
    "            else:\n",
    "                encoded.append(input_bytes[index])\n",
    "                index += 1\n",
    "        \n",
    "        if update_unused:\n",
    "            unused += 1\n",
    "\n",
    "        level += 1\n",
    "        input_bytes = encoded\n",
    "    \n",
    "    return input_bytes\n",
    "\n",
    "def bp_encode_train(input_str:str, max_level=20) -> [str, dict[int,int]]:\n",
    "    dictionary={}\n",
    "    input_bytes = [int(b) for b in input_str.encode(\"utf-8\")]\n",
    "\n",
    "    encoded = bp_encoding(input_bytes, dictionary, max_level=max_level)\n",
    "    return encoded, dictionary\n",
    "\n",
    "def bp_encode(input_str:str, dictionary:dict[int,int]) -> list[int]:\n",
    "    symbols_to_compress = dictionary.keys()\n",
    "    input_bytes = list(input_str.encode(\"utf-8\"))\n",
    "    \n",
    "    for byte_to_compress in symbols_to_compress:\n",
    "        index = 0\n",
    "        new_tokens = []\n",
    "        \n",
    "        while index < len(input_bytes):\n",
    "            if index == len(input_bytes) - 1:\n",
    "                new_tokens.append(input_bytes[index])\n",
    "                break\n",
    "            \n",
    "            b0 = input_bytes[index]\n",
    "            b1 = input_bytes[index + 1]\n",
    "    \n",
    "            packed_bp = b0 | (b1 << 32)\n",
    "    \n",
    "            if packed_bp == byte_to_compress:\n",
    "                new_tokens.append(dictionary[packed_bp])\n",
    "                index += 2\n",
    "            else:\n",
    "                new_tokens.append(b0)\n",
    "                index += 1\n",
    "        \n",
    "        input_bytes = new_tokens\n",
    "\n",
    "    return input_bytes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ab90c877-959f-4362-ada8-2cb4ca338663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_decode(input_tokens: list[int], dictionary: dict) -> list[int]:\n",
    "    decoded_tokens = list(input_tokens)\n",
    "\n",
    "    # Sort the dictionary keys (new symbols) in descending order.\n",
    "    # We must reverse the substitutions, so we start with the last encoded symbol.\n",
    "    symbols_to_expand = sorted(dictionary.keys(), reverse=True)\n",
    "\n",
    "    for symbol in symbols_to_expand:\n",
    "        # Get the packed pair integer for the current symbol\n",
    "        packed_pair = dictionary[symbol]\n",
    "        \n",
    "        # Token_2 (MSB): Shift right by 32\n",
    "        # Token_1 (LSB): Use bitwise AND with 0xFFFF to isolate the lower 32 bits\n",
    "        token_2 = packed_pair >> 32\n",
    "        token_1 = packed_pair & 0xFFFFFFFF\n",
    "        original_pair = [token_1, token_2] # Note: Order is LSB then MSB\n",
    "\n",
    "        new_decoded_tokens = []\n",
    "        i = 0\n",
    "        while i < len(decoded_tokens):\n",
    "            if decoded_tokens[i] == symbol:\n",
    "                # Replace the encoded symbol with its original two tokens\n",
    "                new_decoded_tokens.extend(original_pair)\n",
    "            else:\n",
    "                new_decoded_tokens.append(decoded_tokens[i])\n",
    "            i += 1\n",
    "        \n",
    "        decoded_tokens = new_decoded_tokens\n",
    "\n",
    "    return bytes(decoded_tokens).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4510a-8da0-4b9c-bfc1-16f59c80cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"ðŸ˜€ðŸ˜ƒðŸ˜„ðŸ˜ðŸ˜†ðŸ˜…ðŸ¤£ðŸ˜‚ðŸ™‚ðŸ™ƒ\"\n",
    "print(input_str)\n",
    "e, dictionary = bp_encode_train(contents, max_level=25)\n",
    "rev_dictionary = {v:k for k,v in dictionary.items()}\n",
    "e = bp_encode(input_str, rev_dictionary)\n",
    "print(e)\n",
    "print(bp_decode(e, dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cef93c-36b3-4b8d-9fd4-3b063ddbcc7e",
   "metadata": {},
   "source": [
    "## Numba: Just-In-Time (JIT) Compilation Motivation\n",
    "\n",
    "The library **Numba** is used to accelerate the training, encoding, and decoding portions of the Byte Pair Encoding Python code.\n",
    "\n",
    "### What is Numba?\n",
    "\n",
    "**Numba** is an open-source **Just-In-Time (JIT) compiler** that translates a subset of Python and NumPy code into fast, native **machine code** at runtime using the **LLVM** compiler infrastructure.\n",
    "\n",
    "When a function is decorated with `@numba.njit` (an alias for `@jit(nopython=True)`), Numba:\n",
    "1.  **Infers Types:** Analyzes the Python function and determines the specific data types of all variables.\n",
    "2.  **Compiles:** Translates the Python bytecode directly into optimized machine code, specifically tailored to your CPU.\n",
    "3.  **Executes at Native Speed:** Executes the compiled machine code, which runs **entirely without the Python interpreter's overhead** (in \"nopython mode\").\n",
    "\n",
    "### Why Use Numba?\n",
    "\n",
    "Numba is incredibly useful for speeding up Python code that is typically slow:\n",
    "\n",
    "* **Eliminates Interpreter Overhead:** It bypasses the overhead of the CPython interpreter, which is the main bottleneck for numerical code and explicit loops in standard Python.\n",
    "* **Accelerates Loops and NumPy:** It provides significant speedups for numerical code that heavily relies on `for` loops and NumPy array operations, often achieving **orders of magnitude (10x-100x) speedup** compared to pure Python.\n",
    "* **Parallelism:** It can automatically parallelize certain loops to leverage multiple CPU cores with minimal code changes.\n",
    "\n",
    "### The Trade-off: Code vs. Speed\n",
    "\n",
    "Using Numba involves an engineering **trade-off** between developer effort and execution speed:\n",
    "\n",
    "| Language/Method | Developer Effort | Performance |\n",
    "| :--- | :--- | :--- |\n",
    "| **Pure Python** | Low (maximum flexibility) | Slow (due to interpreter overhead) |\n",
    "| **Numba JIT** | **Low-to-Medium** (add decorator, restrict to Numba-supported Python subset) | **Very Fast** (approaches compiled speeds) |\n",
    "| **Pure C/C++** | Very High (manual memory, C-API, complex compilation) | Fastest (the theoretical ceiling) |\n",
    "\n",
    "The motivation for using Numba is that it provides a **massive performance gain** with **minimal coding overhead**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0e482636-aa44-45ab-9f91-2af7a7291535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit, types\n",
    "from numba.typed import Dict, List\n",
    "\n",
    "@njit\n",
    "def bp_encoding_train_numba(\n",
    "    input_array: List, \n",
    "    dictionary: Dict = None, \n",
    "    unused=256, \n",
    "    level=0, \n",
    "    max_level=20, \n",
    "    vocab_size=50000\n",
    ") -> List:\n",
    "    \n",
    "    # Initialize dictionary if not provided (needed for type stability)\n",
    "    if dictionary is None:\n",
    "        dictionary = Dict.empty(\n",
    "            key_type=types.int64,\n",
    "            value_type=types.int64\n",
    "        )\n",
    "    \n",
    "    current_tokens = input_array\n",
    "\n",
    "    while level < max_level and unused < vocab_size:\n",
    "        len_tokens = len(current_tokens)\n",
    "        \n",
    "        # 1. Calculate the byte pairs (packed_bp)\n",
    "        byte_pair_bin = Dict.empty(\n",
    "            key_type=types.int64, \n",
    "            value_type=types.int64\n",
    "        )\n",
    "        \n",
    "        for i in range(len_tokens - 1):\n",
    "            packed_bp = current_tokens[i] | (current_tokens[i+1] << 32)\n",
    "            \n",
    "            # Numba dictionary operations\n",
    "            if packed_bp not in byte_pair_bin:\n",
    "                byte_pair_bin[packed_bp] = 1\n",
    "            else:\n",
    "                byte_pair_bin[packed_bp] += 1\n",
    "\n",
    "        \n",
    "        # Check for empty tokens or only single pairs\n",
    "        if not byte_pair_bin:\n",
    "            break\n",
    "\n",
    "        # 2. Find the most frequent byte pair (top_bp)\n",
    "        # Numba does not support max(dict, key=dict.get). We must iterate manually.\n",
    "        max_count = 0\n",
    "        top_bp = -1 \n",
    "        for bp, count in byte_pair_bin.items():\n",
    "            if count > max_count:\n",
    "                max_count = count\n",
    "                top_bp = bp\n",
    "\n",
    "        \n",
    "        # If the most frequent pair only appears once, we stop merging.\n",
    "        if max_count == 1:\n",
    "            break\n",
    "\n",
    "        # 3. Replace the top byte pair\n",
    "        encoded_list = List.empty_list(types.int64)\n",
    "        index = 0\n",
    "        update_unused = False\n",
    "        \n",
    "        while index < len_tokens:\n",
    "            if index == len_tokens - 1:\n",
    "                encoded_list.append(current_tokens[index])\n",
    "                break\n",
    "            \n",
    "            # Pack the current pair, should be a pretty fast way of doing this\n",
    "            packed_bp = current_tokens[index] | (current_tokens[index+1] << 32)\n",
    "            \n",
    "            if packed_bp == top_bp:\n",
    "                encoded_list.append(unused)\n",
    "                dictionary[unused] = top_bp\n",
    "                index += 2\n",
    "                update_unused = True\n",
    "            else:\n",
    "                encoded_list.append(current_tokens[index])\n",
    "                index += 1\n",
    "        \n",
    "        # Update state for the next iteration\n",
    "        if update_unused:\n",
    "            unused += 1\n",
    "\n",
    "        level += 1\n",
    "        \n",
    "        current_tokens = encoded_list\n",
    "    \n",
    "    return current_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "98693e54-f866-4d9a-b348-26b175f13039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_encoding_wrapper(input_str: str, max_level=20, vocab_size=50000) -> tuple[List, Dict]:\n",
    "    # 1. Convert the input string to a sequence of byte-level integers (0-255).\n",
    "    byte_sequence = input_str.encode('utf-8')\n",
    "    \n",
    "    input_array_np = np.frombuffer(byte_sequence, dtype=np.uint8).astype(np.int64)\n",
    "    input_array = List(input_array_np)\n",
    "\n",
    "    # 2. Initialize the Numba Typed Dictionary for storing merges.\n",
    "    dictionary = Dict.empty(\n",
    "        key_type=types.int64,\n",
    "        value_type=types.int64\n",
    "    )\n",
    "    \n",
    "    # 3. Call the njit-compiled function\n",
    "    encoded_tokens = bp_encoding_train_numba(\n",
    "        input_array, \n",
    "        dictionary=dictionary, \n",
    "        unused=256, \n",
    "        level=0, \n",
    "        max_level=max_level, \n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "    \n",
    "    return encoded_tokens, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "132aae07-365e-40b2-bb9a-89a029c4b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_encode(input_str:str, dictionary:Dict) -> np.ndarray:\n",
    "    byte_sequence = input_str.encode('utf-8')\n",
    "    input_array_np = np.frombuffer(byte_sequence, dtype=np.uint8).astype(np.int64)\n",
    "    input_array = List(input_array_np)\n",
    "    return bp_encode_numba(input_array, dictionary)\n",
    "\n",
    "@njit\n",
    "def bp_encode_numba(input_array:List, dictionary:Dict) -> List:\n",
    "    symbols_to_compress = dictionary.keys()\n",
    "\n",
    "    for byte_to_compress in symbols_to_compress:\n",
    "        index = 0\n",
    "        new_tokens = List.empty_list(types.int64)\n",
    "        \n",
    "        while index < len(input_array):\n",
    "            if index == len(input_array) - 1:\n",
    "                new_tokens.append(input_array[index])\n",
    "                break\n",
    "            \n",
    "            b0 = input_array[index]\n",
    "            b1 = input_array[index + 1]\n",
    "    \n",
    "            packed_bp = b0 | (b1 << 32)\n",
    "    \n",
    "            if packed_bp == byte_to_compress:\n",
    "                new_tokens.append(dictionary[packed_bp])\n",
    "                index += 2\n",
    "            else:\n",
    "                new_tokens.append(b0)\n",
    "                index += 1\n",
    "        \n",
    "        input_array = new_tokens\n",
    "\n",
    "    return input_array  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "937135f6-3b8f-4c9a-835b-9a53941ddbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def reverse_numba_dict(dictionary: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Creates a new Numba typed Dict with keys and values swapped.\n",
    "    \n",
    "    Args:\n",
    "        dictionary: The original Numba Dict (Token ID -> Packed Pair).\n",
    "        \n",
    "    Returns:\n",
    "        A new Numba Dict (Packed Pair -> Token ID).\n",
    "    \"\"\"\n",
    "    # 1. Create a new empty Dict. The key/value types are swapped compared to the original.\n",
    "    # Original: key=int64 (Token ID), value=int64 (Packed Pair)\n",
    "    # Reverse:  key=int64 (Packed Pair), value=int64 (Token ID)\n",
    "    rev_dictionary = Dict.empty(\n",
    "        key_type=types.int64,\n",
    "        value_type=types.int64\n",
    "    )\n",
    "    \n",
    "    # 2. Iterate manually and assign the swapped key-value pairs\n",
    "    for token_id, packed_pair in dictionary.items():\n",
    "        # packed_pair becomes the key, token_id becomes the value\n",
    "        rev_dictionary[packed_pair] = token_id\n",
    "        \n",
    "    return rev_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "61cb8bdb-a774-4854-8bc8-9369a656180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def bp_decode_numba(input_tokens: List, dictionary: Dict) -> List:\n",
    "    decoded_tokens = input_tokens\n",
    "\n",
    "    # Sort the dictionary keys (new symbols) in descending order.\n",
    "    # We must reverse the substitutions, so we start with the last encoded symbol.\n",
    "    symbols_to_expand = sorted(dictionary.keys(), reverse=True)\n",
    "\n",
    "    for symbol in symbols_to_expand:\n",
    "        # Get the packed pair integer for the current symbol\n",
    "        packed_pair = dictionary[symbol]\n",
    "        \n",
    "        # Token_2 (MSB): Shift right by 32\n",
    "        # Token_1 (LSB): Use bitwise AND with 0xFFFF to isolate the lower 32 bits\n",
    "        token_2 = packed_pair >> 32\n",
    "        token_1 = packed_pair & 0xFFFFFFFF\n",
    "        original_pair = List.empty_list(types.int64)\n",
    "        original_pair.append(token_1) \n",
    "        original_pair.append(token_2) # Note: Order is LSB then MSB\n",
    "\n",
    "        new_decoded_tokens = List.empty_list(types.int64)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(decoded_tokens):\n",
    "            if decoded_tokens[i] == symbol:\n",
    "                # Replace the encoded symbol with its original two tokens\n",
    "                new_decoded_tokens.extend(original_pair)\n",
    "            else:\n",
    "                new_decoded_tokens.append(decoded_tokens[i])\n",
    "            i += 1\n",
    "        \n",
    "        decoded_tokens = new_decoded_tokens\n",
    "\n",
    "    return decoded_tokens\n",
    "\n",
    "def bp_decode(input_tokens: List, dictionary: Dict):\n",
    "    return bytes(bp_decode_numba(input_tokens, dictionary)).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698fd1a-f2dc-485b-8fb1-204b63ec429c",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding (BPE) Training Summary\n",
    "\n",
    "The Byte Pair Encoding (BPE) algorithm was trained on the **Don Quijote de la Mancha** corpus to create an efficient subword tokenizer. The goal was to balance a manageable vocabulary size with better text compression than character-level tokenization.\n",
    "\n",
    "* **Total Merge Iterations:** The BPE training was executed for a maximum of **10,000 merge operations** (`max_level=10000`).\n",
    "* **Final Vocabulary Size:** The training process resulted in a total learned vocabulary size of **10,255** tokens (determined by the base 255 byte tokens plus the 10,000 merges).\n",
    "    * This is confirmed by the output: `vocab_size: 10255`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "140b3960-f00c-4a4c-b8dc-b6e887af206e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10255\n"
     ]
    }
   ],
   "source": [
    "final_tokens, merge_dict = bp_encoding_wrapper(contents, max_level=10000)\n",
    "merge_dict_rev = reverse_numba_dict(merge_dict)\n",
    "print(f\"vocab_size: {max(merge_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f006e781-6784-4454-8939-93e615705b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Object successfully serialized and saved to './bpe_encoding_vocab.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "file_path = \"./bpe_encoding_vocab.pkl\"\n",
    "try:\n",
    "    with open(file_path, 'wb') as file:\n",
    "        # Save the dictionary containing all data to the file\n",
    "        pickle.dump(dict(merge_dict), file)\n",
    "    print(f\"âœ… Object successfully serialized and saved to '{file_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during serialization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "471e414c-9d7a-45d8-91af-7005d4b02e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "\n",
    "def generate_ansi_colors(N: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates N distinct ANSI 256-color escape codes for terminal foreground coloring.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Total number of colors available in the 6x6x6 color cube\n",
    "    MAX_COLORS = 216\n",
    "    \n",
    "    if N > MAX_COLORS:\n",
    "        print(f\"Warning: Cannot generate {N} distinct colors from the 216-color cube.\")\n",
    "        N = MAX_COLORS\n",
    "        \n",
    "    START_CODE = 16\n",
    "    \n",
    "    # Calculate the step size to space the N colors evenly across the available range\n",
    "    step = MAX_COLORS // N if N > 0 else 1\n",
    "    \n",
    "    ansi_codes = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Calculate the color index (16 to 231)\n",
    "        color_index = START_CODE + (i * step)\n",
    "        \n",
    "        # Ensure the index doesn't exceed the color cube range (231)\n",
    "        if color_index > 231:\n",
    "            color_index = START_CODE + (color_index % (231 - START_CODE))\n",
    "        \n",
    "        ansi_code = f'\\033[48;5;{color_index}m'\n",
    "        ansi_codes.append(ansi_code)\n",
    "        \n",
    "    return ansi_codes\n",
    "\n",
    "NUM_COLORS = 50\n",
    "RESET = '\\033[0m'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7eb850-554a-4d08-94d0-1c35438ea708",
   "metadata": {},
   "source": [
    "## Tokenizer Visualization and Language Efficiency\n",
    "\n",
    "The visualization cell demonstrates the fundamental efficiency of a tokenizer trained on a specific language (in this case, **Spanish**) when processing text in that language, versus a new language (English).\n",
    "\n",
    "### Why Spanish Compresses Better\n",
    "\n",
    "The BPE training is a **greedy, data-driven process**. It learns to combine the most frequent adjacent byte/token pairs in the training corpus into new, single subword tokens.\n",
    "\n",
    "1.  **High-Frequency Merges (Spanish):** The training text is entirely in Spanish. Consequently, the tokenizer learns common Spanish pairs and subwords like `por`, `que`, `el`, `cielo`, and `azul?`.\n",
    "    * When the Spanish sentence **\"por que es el cielo azul?\"** is tokenized, the BPE algorithm finds many existing, learned subword tokens, resulting in a **short, compressed sequence** (e.g., 7 tokens in the notebook's example).\n",
    "2.  **Low-Frequency Merges (English):** When an English sentence like **\"why is the sky blue\"** is passed to the same tokenizer:\n",
    "    * Some common English subwords are **not** present in the learned Spanish dictionary.\n",
    "    * The BPE algorithm falls back to the smaller, less efficient learned tokens, often resulting in sequences that are closer to **character-level tokenization**.\n",
    "    * This yields a **longer, less compressed sequence** (more tokens) for the same number of conceptual \"words,\" increasing the sequence length ($T$) and the associated $O(T^2)$ computational cost of the self-attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "fe27d843-c179-47da-b85d-570f8f86bcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;255mporâ€¢\u001b[0m\u001b[38;2;255;191;0mqueâ€¢\u001b[0m\u001b[38;2;127;255;0mesâ€¢elâ€¢\u001b[0m\u001b[38;2;0;63;255mcieloâ€¢\u001b[0m\u001b[38;2;255;0;0maz\u001b[0m\u001b[38;2;0;255;63mul\u001b[0m\u001b[38;2;127;0;255m?\u001b[0m\n",
      "[337, 267, 1485, 1783, 417, 750, 63]\n",
      "\u001b[38;2;191;0;0mwh\u001b[0m\u001b[38;2;0;63;255myâ€¢\u001b[0m\u001b[38;2;127;0;255misâ€¢\u001b[0m\u001b[38;2;255;0;0mtheâ€¢\u001b[0m\u001b[38;2;255;0;191ms\u001b[0m\u001b[38;2;0;255;255mk\u001b[0m\u001b[38;2;0;63;255myâ€¢\u001b[0m\u001b[38;2;255;191;0mb\u001b[0m\u001b[38;2;0;255;63mlu\u001b[0m\u001b[38;2;127;255;0me\u001b[0m\u001b[38;2;191;143;0m?\u001b[0m\n",
      "[10228, 272, 592, 1313, 115, 107, 272, 98, 489, 101, 63]\n"
     ]
    }
   ],
   "source": [
    "input_str = None\n",
    "\n",
    "if not input_str:\n",
    "    input_str = \"por que es el cielo azul?\"\n",
    "\n",
    "def visualize_tokenizer(input_str):\n",
    "    encoded_str = bp_encode(input_str,  merge_dict_rev)\n",
    "    color_list = generate_diverse_true_color_ansi_codes(NUM_COLORS)\n",
    "    color_map = dict(zip(set(encoded_str), color_list))\n",
    "    \n",
    "    t = {}\n",
    "    for k, v in color_map.items():\n",
    "        token = List([k])\n",
    "        word = bp_decode(token, merge_dict)\n",
    "        t[k] = word\n",
    "    \n",
    "    res = \"\"\n",
    "    for token in encoded_str:\n",
    "        token = token\n",
    "        word = t[token].replace(\" \",\"â€¢\")\n",
    "        res += f\"{color_map[token]}{word}{RESET}\"\n",
    "    \n",
    "    print(res)\n",
    "    print(encoded_str)\n",
    "\n",
    "visualize_tokenizer(\"por que es el cielo azul?\")\n",
    "visualize_tokenizer(\"why is the sky blue?\")\n",
    "\n",
    "#r = input(\"Tokenizer visualisation:\\ntry:\\\"por que es el cielo azul?\\\"\\nthen: \\\"try a sentence in english!\\\"\\n\")#\"Por que es el cielo azul?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeebf4b-2dc6-452b-9539-905713505118",
   "metadata": {},
   "source": [
    "## Rotary Positional Embeddings (RoPE)\n",
    "\n",
    "The `grm` function is an implementation of the core mathematical component of **Rotary Positional Embeddings (RoPE)**, a technique introduced by the LLaMA family of models to efficiently integrate positional information into the self-attention mechanism, replacing the traditional absolute positional embeddings.\n",
    "\n",
    "Unlike traditional positional encodings, RoPE applies a **rotation** to the Query ($\\boldsymbol{Q}$) and Key ($\\boldsymbol{K}$) vectors based on the token's absolute position in the sequence ($m$).\n",
    "\n",
    "### Calculating the Rotation Frequencies\n",
    "\n",
    "The core of RoPE is the rotation angle ($\\theta_i$) for each dimension $i$ of the embedding. This is done by calculating a base frequency vector based on the odd/even indices of the embedding dimension.\n",
    "\n",
    "1.  **Base Frequency Term:** The term $\\frac{1}{\\theta^{\\frac{i}{d}}}$ is computed, where:\n",
    "    * $i$ is the index of the embedding dimension, which is iterated over as $0, 2, 4, \\dots, d-2$.\n",
    "    * $d$ is the full embedding dimension $C$ (`n_embd`).\n",
    "    * $\\theta$ is a constant base (e.g., $10000$ or $\\pi$ as shown in the tests).\n",
    "2.  **Code Implementation (`grm` function):**\n",
    "    * `i_indices = torch.arange(0, n_embd, 2)`: Generates the $i$ indices ($0, 2, \\dots$).\n",
    "    * `frequencies_to_rotate = 1.0 / (theta ** (i_indices / n_embd))`: Computes the base frequency term $\\frac{1}{\\theta^{\\frac{i}{d}}}$.\n",
    "    * `m = torch.arange(len_sequence)`: Generates the token position indices $m$ ($0, 1, 2, \\dots, T-1$).\n",
    "    * `all_frequencies = torch.outer(m, frequencies_to_rotate)`: Calculates the final rotation angle for every dimension $i$ and every position $m$:\n",
    "        $$\\phi_{m, i} = m \\cdot \\left(\\frac{1}{\\theta^{\\frac{i}{d}}}\\right)$$\n",
    "3.  **Rotation Matrix Components:**\n",
    "    * The `cos_rot` and `sin_rot` matrices are then computed as $\\cos(\\phi_{m, i})$ and $\\sin(\\phi_{m, i})$.\n",
    "    * The shape of both `cos_rot` and `sin_rot` is $(\\boldsymbol{T}, \\boldsymbol{C}/\\mathbf{2})$, where $T$ is `len_sequence` and $C/2$ is the number of paired rotation indices.\n",
    "\n",
    "### Avoiding the Full Rotation Matrix\n",
    "\n",
    "A naive, explicit implementation of RoPE would calculate the full 2D rotation matrix $R_m$ of size $(C, C)$ for every position $m$ and then multiply the input vector by it, $Q'_m = R_m Q_m$.\n",
    "\n",
    "$$R_m = \\begin{pmatrix} \\cos(\\phi_{m, 0}) & -\\sin(\\phi_{m, 0}) & 0 & 0 & \\dots \\\\ \\sin(\\phi_{m, 0}) & \\cos(\\phi_{m, 0}) & 0 & 0 & \\dots \\\\ 0 & 0 & \\cos(\\phi_{m, 2}) & -\\sin(\\phi_{m, 2}) & \\dots \\\\ 0 & 0 & \\sin(\\phi_{m, 2}) & \\cos(\\phi_{m, 2}) & \\dots \\\\ \\dots & \\dots & \\dots & \\dots & \\dots \\end{pmatrix}$$\n",
    "\n",
    "The **optimized implementation** (as shown in the notebook's test cells) avoids this large matrix multiplication by exploiting the block-diagonal structure of $R_m$ and performing the rotation in place:\n",
    "\n",
    "1.  **Split Vectors:** The input vector $x$ (Query or Key) is split into two halves: the **even indices** $x_{\\text{even}}$ (dimensions $0, 2, \\dots$) and the **odd indices** $x_{\\text{odd}}$ (dimensions $1, 3, \\dots$).\n",
    "2.  **Apply Rotation Formula:** The rotation is applied directly using the pre-calculated `cos_rot` and `sin_rot` (which are of shape $(T, C/2)$):\n",
    "    $$\\text{Rotated}(x_0) = x_0 \\cos(\\phi) - x_1 \\sin(\\phi)$$\n",
    "    $$\\text{Rotated}(x_1) = x_1 \\cos(\\phi) + x_0 \\sin(\\phi)$$\n",
    "3.  **Combine Rotated Halves:** The newly rotated even and odd vectors are interleaved back into the final vector $\\boldsymbol{x}'$.\n",
    "\n",
    "This approach ensures the positional encoding is applied with a **minimal number of multiplications and without the need to instantiate large rotation matrices**, significantly improving efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "6fd98dd3-2f47-4b5e-858a-e7270307ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grm(len_sequence:int, n_embd:int, theta:float=10000):\n",
    "    i_indices = torch.arange(0, n_embd, 2)\n",
    "    m = torch.arange(len_sequence)\n",
    "    frequencies_to_rotate = 1.0 / (theta ** (i_indices / n_embd))\n",
    "\n",
    "    all_frequencies = torch.outer(m, frequencies_to_rotate)\n",
    "    cos_rot = torch.cos(all_frequencies)\n",
    "    sin_rot = torch.sin(all_frequencies)\n",
    "    return cos_rot, sin_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "70054a33-5555-4ce1-83d0-fe3145c4f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "n_embd = 8\n",
    "cos_rot, sin_rot = grm(2, n_embd, math.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "f288a9e4-745c-400e-b566-e98993bc653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0,1,2,3,4,5,6,7], [0,1,2,3,4,5,6,7]])\n",
    "x_even = x[..., 0::2]\n",
    "x_odd  = x[..., 1::2]\n",
    "x_even_rot = (cos_rot * x_even - sin_rot * x_odd).bfloat16()\n",
    "x_odd_rot = (cos_rot * x_odd + sin_rot * x_even).bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "1af89545-4028-465f-a8ff-a4236a23ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rotated_x_slice = torch.empty((2, n_embd), dtype=x_even_rot.dtype) \n",
    "# 2. Assign the rotated even indices to the even columns (0, 2, 4, 6)\n",
    "final_rotated_x_slice[:, 0::2] = x_even_rot \n",
    "# 3. Assign the rotated odd indices to the odd columns (1, 3, 5, 7)\n",
    "final_rotated_x_slice[:, 1::2] = x_odd_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "f3d0d0fa-f8dd-44ca-86e8-5ce65c997811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  2.0000,  3.0000,  4.0000,  5.0000,  6.0000,  7.0000],\n",
       "        [-0.8398,  0.5391, -0.5859,  3.5625,  0.7070,  6.3750,  2.5938,  8.8750]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rotated_x_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6e23d4ff-8568-41e5-a415-fe765f62e7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.531983 M parameters\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4\n",
    "n_embd = 1024 >> 2\n",
    "num_heads = 12\n",
    "block_size = 2048 >> 2\n",
    "vocab_size = max(merge_dict)\n",
    "gpt = NanoGPT(num_layers, vocab_size, n_embd, num_heads, block_size)\n",
    "gpt.to(device)\n",
    "print(sum(p.numel() for p in gpt.parameters())/1e6, 'M parameters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
