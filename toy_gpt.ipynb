{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17be996a-71ef-41c8-825a-e8a8a18ed706",
   "metadata": {},
   "source": [
    "# Building a NanoGPT from Scratch: Attempting to Understand the Transformer Architecture\n",
    "## 0. Introduction\n",
    "My journey began in late 2024/early 2025 when I discovered Andrej Karpathy's video **Let's build GPT: from scratch, in code, spelled out**. The two-hour deep dive provided a clear and, low-level overview of the architecture. While I grasped the general framework of the Transformer, the inner workings of the Attention mechanism remained opaque.\n",
    "\n",
    "The challenge wasn't just the architecture itself; it was my **foundation**. My memories of undergraduate linear algebra, & machine learning were nonexistent, as if by now those tokens were outside of my context window. necessitating a serious refresh on core concepts like **matrix multiplication, dot product, loss function, backpropagation, and tensor dimensionality**.\n",
    "\n",
    "Throughout the year, I repeatedly returned to Karpathyâ€™s explanation. **Each time, I peeled back a new layer of complexity**, steadily improving my grasp of the Transformer's operation. What started as a vague concept slowly transformed (hehe) into a detailed understanding.\n",
    "\n",
    "This document is the culmination of that effort my attempt to synthesize everything Iâ€™ve learned about how these **\"thinking machines\"** work. I aimed for clarity and depth, and I hope it proves genuinely useful to others starting this same path.\n",
    "\n",
    "This notebook provides a technical overview and guide for a minimal implementation of a Generative Pre-trained Transformer (**NanoGPT**), built using PyTorch. This project serves as a clear, educational reference for understanding the core components of modern large language models, from data preparation to the self-attention mechanism and text generation.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Data Acquisition and Character-Level Tokenization\n",
    "\n",
    "The first step in any language modeling project is preparing the text data. For this notebook, we used a public domain work of literature: **Don Quijote de la Mancha**.\n",
    "\n",
    "### Data Loading and Tokenization\n",
    "The text was downloaded directly from Project Gutenberg using `curl`. A simple **character-level tokenizer** was implemented by mapping every unique character to an integer ID.\n",
    "\n",
    "* The **vocabulary size** ($\\boldsymbol{V}$ or `vocab_size`) is determined by the number of unique characters.\n",
    "* The entire corpus is encoded into a long list of integers.\n",
    "\n",
    "### Data Preparation and Dimensions\n",
    "The encoded corpus is split into training and validation sets (90/10). For training, we sample batches of input sequences ($X$) and target sequences ($Y$).\n",
    "\n",
    "Let $B$ be the batch size (which is implicitly set to `n_embd = 64` in the `sample_data` call for the loss estimate) and $T$ be the context length (`block_size = 32`).\n",
    "\n",
    "* **Input Data:** The input batch $X$ is a tensor of shape $(B, T)$.\n",
    "* **Target Data:** The target batch $Y$ is also a tensor of shape $(B, T)$, where $Y_{i, t}$ is the correct prediction for the token $X_{i, t+1}$.\n",
    "\n",
    "### Sources\n",
    "\n",
    "* **Attention Is All You Need**\n",
    "* **LLM Visualization**\n",
    "* **Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch**\n",
    "* **Andrej Karpathy's makemore/nanoGPT Video**\n",
    "* **Language Models are Unsupervised Multitask Learners (GPT-2 Paper)**\n",
    "* **Wikipedia: Byte-pair encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d3a2c76-aa67-4690-9b7c-e7267f1bc927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2173k  100 2173k    0     0  1801k      0  0:00:01  0:00:01 --:--:-- 1802k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://www.gutenberg.org/files/2000/2000-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0e0fa6-016a-4e68-9e73-c2863b37527f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/stephan.meza/ToyGPT/toy_gpt/bin/python3: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/stephan.meza/ToyGPT/toy_gpt/bin/python3: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd380d93-6a6f-418e-be13-33a9ebf29631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fa9757-8385-4ed0-bd76-2a461a7bac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./2000-0.txt\", \"r\") as f:\n",
    "    contents = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "656e554b-7c65-453f-8bb7-b3d14bb2312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noso y grande escrutinio que el cura y el\n",
      "barbero hicieron en la librerÃ­a de nuestro ingenioso hidalgo\n",
      "\n",
      "De la segunda salida de nuestro buen caballero don\n",
      "Quijote de la Mancha\n",
      "\n",
      "Del buen suceso que el valeroso don Quijote tuvo en\n",
      "la espantable y jamÃ¡s imaginada aventura de los molinos de viento, con\n",
      "otros sucesos dignos de felice recordaciÃ³n\n",
      "\n",
      "Donde se concluye y da fin a la estupenda batalla que\n",
      "el gallardo vizcaÃ­no y el valiente manchego tuvieron\n",
      "\n",
      "De lo que mÃ¡s le avino a don Quijote con el vizc\n"
     ]
    }
   ],
   "source": [
    "num_tokens = 500\n",
    "offset = 3\n",
    "print(contents[num_tokens*offset:num_tokens*offset+num_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e98ede-0e4a-4e4f-af69-d0faab15e3bd",
   "metadata": {},
   "source": [
    "### Character-Level Tokenization and Vocab Creation\n",
    "\n",
    "We first create a simple **character-level tokenizer**. This approach is simple and highly robust (never fails on an OOV word) but is computationally inefficient for large models due to sequence bloat.\n",
    "\n",
    "1.  **Extract Vocabulary:** Find all unique characters in the corpus.\n",
    "2.  **Mapping:** Create lookup tables (`stoi` for string-to-integer and `itos` for integer-to-string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64fb421c-0302-4f1a-8625-1353d0d02ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "chars = sorted(list(set(contents)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ca1749-72ca-4d5e-af89-5db1a9ca71c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 61, 68, 68, 71, 2]\n",
      "hello!\n"
     ]
    }
   ],
   "source": [
    "a = \"hello!\"\n",
    "print(encode(a))\n",
    "print(decode(encode(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebeea29-bab1-49b2-a663-51dd6510e559",
   "metadata": {},
   "source": [
    "### Batching Function\n",
    "\n",
    "The **Transformer** operates on fixed-length chunks. We define a function to sample random batches of input ($X$) and target ($Y$) sequences.\n",
    "\n",
    "* **Context Length ($\\boldsymbol{T}$):** `block_size` (e.g., 32) is the maximum sequence length the model processes at once.\n",
    "* **Batch Size ($\\boldsymbol{B}$):** The number of independent sequences processed in parallel.\n",
    "\n",
    "The target $\\boldsymbol{Y}$ is simply the input $\\boldsymbol{X}$ shifted one position to the right, as the model predicts the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e19bd19-b115-4401-8a22-f1172f7abe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(training_data: str, split_ratio: float = 0.9) -> tuple[str,str]:\n",
    "    training_data_length = len(training_data)\n",
    "    marker = int(split_ratio * training_data_length)\n",
    "    return training_data[:marker], training_data[marker:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57d0de4e-e662-40e3-9b70-a28b53ea36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def sample_data(data, len_samples:int=16, num_samples:int=8) -> torch.Tensor:\n",
    "    len_data = len(data)\n",
    "    random_indices = torch.randint(0, len_data - len_samples, (num_samples,))\n",
    "    sample_x = torch.stack([\n",
    "        torch.tensor(data[r_ind:r_ind+len_samples]) \n",
    "        for r_ind in random_indices\n",
    "    ])\n",
    "    sample_y = torch.stack([\n",
    "        torch.tensor(data[r_ind+1:r_ind+1+len_samples]) \n",
    "        for r_ind in random_indices\n",
    "    ])\n",
    "    return sample_x, sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd0520d4-7554-4e8e-8118-3e273b0c4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_contents = encode(contents)\n",
    "training_data, validation_data = split_data(encoded_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3895a97-46e8-43d7-9e9f-b15c602a5234",
   "metadata": {},
   "source": [
    "## 2. The Transformer Block and Attention Mechanism\n",
    "\n",
    "The core of the NanoGPT is the **Transformer Block**, based on the seminal \"Attention Is All You Need\" paper. This is a decoder-only architecture, characteristic of GPT models.\n",
    "\n",
    "### The NanoGPT Architecture\n",
    "The model uses several learned components to transform the input:\n",
    "1.  **Token Embeddings:** Maps the input IDs $(B, T)$ to a continuous space of size $C$ (`embedding_dim = 2048`).\n",
    "    $$\\text{Token Embeddings}: (B, T) \\rightarrow (B, T, C)$$\n",
    "2.  **Positional Embeddings:** A learned encoding added to the token embeddings to provide sequential context, as attention itself is order-agnostic.\n",
    "3.  **Final Input Vector:** The sum of the token and positional embeddings forms the final input vector to the blocks.\n",
    "\n",
    "### Causal Self-Attention and Matrix Sizes\n",
    "The `AttentionHead` implements the scaled dot-product attention, modified for causality (GPT-style).\n",
    "\n",
    "Let $H$ be the size of a single attention head (`head_size`). $H = C / N_h$, where $N_h$ is the number of heads (`num_heads = 12`).\n",
    "\n",
    "1.  **Linear Projections (Q, K, V):** The input vector $x$ (shape $(B, T, C)$) is projected into Query ($\\boldsymbol{Q}$), Key ($\\boldsymbol{K}$), and Value ($\\boldsymbol{V}$) matrices.\n",
    "    $$\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V} = W_Q(x), W_K(x), W_V(x) \\rightarrow (B, T, H)$$\n",
    "2.  **Attention Logits ($\\Omega$):** The raw attention scores ($\\Omega$) are calculated by the dot product of $Q$ and $K^T$. This score determines how much token $i$ should pay attention to token $j$.\n",
    "    $$\\Omega = \\boldsymbol{Q}\\boldsymbol{K}^T \\rightarrow (B, T, H) \\times (B, H, T) \\rightarrow (B, T, T)$$\n",
    "3.  **Scaling and Masking:** $\\Omega$ is scaled by $\\frac{1}{\\sqrt{H}}$ to stabilize gradients. A lower-triangular mask (`tril`) is applied, setting entries corresponding to future tokens to $-\\infty$. This is the **causal mask**, enforcing that the prediction for token $t$ can only depend on tokens $1$ through $t$.\n",
    "4.  **Attention Weights ($W$):** The masked logits are passed through a softmax function, resulting in the attention weights $W$. These weights are the probability distribution over past tokens for the current token.\n",
    "    $$W = \\text{softmax}(\\Omega_{\\text{masked}} / \\sqrt{H}) \\rightarrow (B, T, T)$$\n",
    "5.  **Context Vector (Output):** The weights $W$ are multiplied by the Value matrix $V$ to create the context vector, which is the output of the attention head.\n",
    "    $$\\text{Output} = W\\boldsymbol{V} \\rightarrow (B, T, T) \\times (B, T, H) \\rightarrow (B, T, H)$$\n",
    "\n",
    "The **Multi-Head Attention** module runs $N_h$ heads in parallel, concatenates their outputs (shape $(B, T, N_h \\cdot H)$), and projects the result back to $C$ using a final linear layer.\n",
    "\n",
    "The **TransformerBlock** wraps this, applying Layer Normalization (Post-LN style) and **residual connections**: $x' = x + \\text{LayerNorm}(\\text{MultiHead}(x))$. This structure ensures gradients can flow easily through the network.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "296f304d-8f1d-43ca-abf4-9344652c45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, n_embd:int, head_size:int, block_size:int, dropout:int=0):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.W_query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.W_key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.W_value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B,T,C = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        omega = queries @ keys.mT # * C**-0.5\n",
    "\n",
    "        omega_masked = omega.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        attention_weights = nn.functional.softmax(omega_masked / self.head_size**0.5, dim=-1)\n",
    "        wei = self.dropout(attention_weights)\n",
    "\n",
    "        context_vector = wei @ values # (num_samples, num_words, num_embeddings_per_word)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self, num_heads:int, n_embd:int, head_size:int, block_size:int, dropout:int=0.0):\n",
    "        super().__init__()\n",
    "        self.attention_blocks = nn.ModuleList([AttentionHead(n_embd, head_size, block_size) for _ in range(num_heads)])\n",
    "        self.linear_projection = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        attention_weights = torch.cat([head(x) for head in self.attention_blocks], dim=-1)\n",
    "        lin = self.linear_projection(attention_weights)\n",
    "        \n",
    "        return self.dropout(lin)\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, n_embd:int, block_size:int, dropout:int=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.Linear(n_embd, 4 * n_embd)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ln2 = nn.Linear(4 * n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.ln1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.ln2(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads:int, n_embd:int, head_size:int, block_size:int):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiAttentionHead(num_heads, n_embd, head_size, block_size)\n",
    "        self.ffn = FeedForwardNetwork(n_embd, block_size)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #x = x + self.multi_head_attention(self.ln1(x))\n",
    "        #x = x + self.ffn(self.ln2(x))\n",
    "\n",
    "        x = x + self.ln1(self.multi_head_attention(x))\n",
    "        x = x + self.ln2(self.ffn(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "274e8d37-ae86-4717-bc08-d079db553c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very simple and stupid GPT model!\n",
    "class NanoGPT(nn.Module):\n",
    "    def __init__(self, num_layers:int, vocab_size:int, embedding_dim:int, num_heads:int, block_size:int):\n",
    "        super().__init__()\n",
    "        head_size = embedding_dim // num_heads\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.positional_embeddings = nn.Embedding(block_size,embedding_dim)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(num_heads, embedding_dim, head_size, block_size) for _ in range(num_layers)])\n",
    "    \n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, targets: torch.Tensor|None=None) -> torch.Tensor:\n",
    "        B,T = input_ids.shape\n",
    "        word_embeddings = self.embedding(input_ids)\n",
    "        pos = torch.arange(T)\n",
    "\n",
    "        positional_embeddings = self.positional_embeddings(pos)\n",
    "\n",
    "        final_vector = word_embeddings + positional_embeddings\n",
    "\n",
    "        x = self.blocks(final_vector) # (B,T,C)\n",
    "        x = self.ln1(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        num_samples, num_words, num_embeddings = logits.shape\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(num_samples * num_words, num_embeddings)\n",
    "            targets = targets.view(num_samples * num_words)\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = nn.functional.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4820f12f-a245-479d-88f7-6e54cd339743",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_data, val_data, block_size, n_embd, eval_iters: int = 20):\n",
    "    # Dictionaries to store the average loss for each split\n",
    "    out = {}\n",
    "    model.eval() # Set model to evaluation mode (crucial for dropout/BatchNorm)\n",
    "\n",
    "    for split_name, data in [('train', train_data), ('val', val_data)]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            # Sample data from the current split (train or val)\n",
    "            xb, yb = sample_data(data, block_size, n_embd)\n",
    "            # Forward pass to get logits and loss\n",
    "            logits, loss = model(xb, yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[split_name] = losses.mean()\n",
    "\n",
    "    model.train() # Set model back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4fab10-f77d-471d-82c1-becdbc3bb952",
   "metadata": {},
   "source": [
    "## 3. Training Optimization: Loss and Gradient Descent\n",
    "\n",
    "### Loss Function\n",
    "The model's final layer, the language model head (`lm_head`), produces **logits** of shape $(B, T, V)$. These logits represent the raw, unnormalized scores for every possible next character in the vocabulary at every time step.\n",
    "\n",
    "The model uses the standard **Cross-Entropy Loss** for classification tasks, comparing the predicted logits against the true target tokens $Y$.\n",
    "\n",
    "$$\\text{Loss} = \\text{CrossEntropyLoss}(\\text{Logits}, \\text{Targets})$$\n",
    "\n",
    "To calculate this, the logits and targets are often reshaped into two-dimensional vectors, effectively treating all $B \\times T$ predictions in the batch as independent classification problems:\n",
    "* $\\text{Logits}: (B \\cdot T, V)$\n",
    "* $\\text{Targets}: (B \\cdot T)$\n",
    "\n",
    "### Gradient Descent and Optimization\n",
    "The goal of training is to find the model parameters (weights) that minimize this loss. This is achieved using **Gradient Descent**, specifically the **AdamW optimizer**.\n",
    "\n",
    "1.  **Forward Pass:** Calculate the loss.\n",
    "2.  **Backward Pass (Backpropagation):** The `loss.backward()` call computes the gradient (derivative) of the loss with respect to every single parameter in the model.\n",
    "3.  **Parameter Update:** The `optimizer.step()` function uses these calculated gradients to update the model's parameters in the direction that decreases the loss, aiming for convergence.\n",
    "\n",
    "The model, with approximately **0.308329 million parameters**, achieved an initial loss of $4.86$ and dropped to $1.71$ after 1000 steps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b1581ec-be1a-4de1-84fc-bed04746a629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.308329 M parameters\n"
     ]
    }
   ],
   "source": [
    "num_layers = 6\n",
    "n_embd = 64\n",
    "num_heads = 12\n",
    "block_size = 32\n",
    "gpt = NanoGPT(num_layers, vocab_size, n_embd, num_heads, block_size)\n",
    "print(sum(p.numel() for p in gpt.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "15e4923b-8aa0-4e56-982b-c2908b21d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8657708168029785\n",
      "2.3796846866607666\n",
      "2.2073700428009033\n",
      "2.044827699661255\n",
      "1.935045599937439\n",
      "1.8543068170547485\n",
      "1.9088845252990723\n",
      "1.8190929889678955\n",
      "1.7947200536727905\n",
      "1.7648370265960693\n",
      "1.7102956771850586\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3)\n",
    "for steps in range(1000): # increase number of steps for good results...\n",
    "    # sample a batch of data\n",
    "\n",
    "    xb, yb = sample_data(training_data, block_size, n_embd)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if steps % 100 == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aa5b274f-6e12-4776-a15c-d2e7046caac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "demore a ella vecÃ­n, por conferÃ­a, se lÃ¡ poder por cemo vol estido hasto le dejado.\n",
      "\n",
      "â€” No, se\n",
      "ellÃ³ â€”rinos sevillos fue cosamentia, amo su soyo de rudir se todor quien a llamantillap unto! â€” que se de vuestro!,\n",
      "y a mer como Sancho. Fermo que va\n",
      "esto, puje don QuamÃ³ tuficÃ³n don Quijote que me soy promera don Quijote de mÃ¡s\n",
      "mujorsiÃ³n a jocas que eja y o caballere,\n",
      "querÃ¡ la\n",
      "demas al que se le humbre a mi Ã­nsensa hos y ni fuermar e tumbizna don Jurad a dal en pinsÃ³ y lo que habita?\n",
      "\n",
      "Â»Â¡Â¡A armÃ¡sfuda de nola\n",
      "pellerÃ­r en nuego los apreso para a con acorgarle.\n",
      " QuÃ©jote, os de\n",
      "asÃ­ dio a mÃ­ podre adellÃ­ \n",
      "que; Â¡Cos Da por desea le que dir aquel con la vanta a masmo, mis\n",
      "alizo no este criÃ©ndono sabijo, antuendo venÃ­a o dos que curra, a estaba, aunque mi se\n",
      "erÃ¡n estracas, y que mi los suceles pensaco y al cuanto.\n",
      "El roblar en los pliguesa\n",
      "de cambieno, que sabÃ­a\n",
      "irtento; dos farre a su estÃ¡ trengÃ¡ndo el corracÃ­do\n",
      "el mÃ¡s de\n",
      "camina ha del Bistar la que homos puceda, otro de\n",
      "vuelcarrime, que lo quÃ© tÃ­an \n"
     ]
    }
   ],
   "source": [
    "print(decode(gpt.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a3875-cd76-4122-8a79-b4e5bd094011",
   "metadata": {},
   "source": [
    "## 4. Why Character-Level Tokenization is Inefficient\n",
    "\n",
    "The character-level approach, while simple and robust against out-of-vocabulary (OOV) words, is highly **inefficient** for training and running modern Transformer-based LLMs:\n",
    "\n",
    "1.  **Excessive Sequence Length (Sequence Bloat):** A single common word requires multiple tokens (characters). This maximizes the sequence length ($T$) for any given text. Texts encoded at the character level are often **3 to 5 times longer** than those encoded using subword methods.\n",
    "2.  **Quadratic Computational Cost:** The self-attention mechanism, the core operation of the Transformer, has a computational complexity that is **quadratic** with respect to the sequence length, $O(T^2)$. If the sequence length is four times longer, the computation for attention is $\\sim 16$ times slower.\n",
    "3.  **Reduced Effective Context Window:** Since LLMs have a fixed maximum context length, excessive tokenization limits the number of actual words the model can \"see\" at once, hindering its ability to capture **long-range dependencies** necessary for high-quality language understanding and generation.\n",
    "4.  **Low Information Density:** Individual character tokens carry very **low semantic meaning**. The model must dedicate substantial capacity to learning the basic compositional structure of words rather than focusing on high-level linguistic relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Next Steps: Implementing Byte Pair Encoding (BPE)\n",
    "\n",
    "To overcome the inefficiencies of character-level tokenization, an implementation of the **Byte Pair Encoding (BPE)** algorithm was written as the next step. BPE, as used in models like GPT-2, balances character-level flexibility with word-level efficiency.\n",
    "\n",
    "The BPE implementation in the notebook followed the general algorithm described on **Wikipedia**:\n",
    "1.  **Iterative Merge:** Start with the input string represented by bytes.\n",
    "2.  **Substitution:** Recursively identify the most frequent adjacent pair of tokens and replace them with a new, single token ID (starting from value 256).\n",
    "3.  **Dictionary:** The substitution is recorded in a `dictionary` mapping the new token ID to the two tokens it replaces, allowing for decoding.\n",
    "\n",
    "### Note on GPT-2 Paper and BPE\n",
    "The implemented BPE was a pure algorithmic approach and **did not** incorporate the specific byte-level exception mentioned in the GPT-2 paper. This exception is typically added to prevent the merging of common words with preceding or trailing punctuation (e.g., separating `dog` from `.`, `!`, or `?`) to preserve valuable vocabulary slots and improve compression efficiency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75bebeb-f524-4949-873b-ad1b5ae71621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_encoding(input_bytes:list[bytes], dictionary={}, unused=256, level=0, max_level=20)-> list[int]:\n",
    "    len_bytes = len(input_bytes)\n",
    "    byte_pair_bin = {}\n",
    "\n",
    "    # here walk through the input and calculate the byte pairs\n",
    "    for i in range(len_bytes-1):\n",
    "        packed_bp = input_bytes[i] | (input_bytes[i+1] << 32)\n",
    "\n",
    "        if packed_bp not in byte_pair_bin:\n",
    "            byte_pair_bin[packed_bp] = 1\n",
    "        else:\n",
    "            byte_pair_bin[packed_bp] += 1\n",
    "\n",
    "    # here we walk through the input in order to replace the byte pair\n",
    "    index = 0\n",
    "    top_bp = max(byte_pair_bin, key=byte_pair_bin.get)\n",
    "\n",
    "    if byte_pair_bin[top_bp]== 1 or level >= max_level:\n",
    "        return input_bytes\n",
    "    \n",
    "    encoded = []\n",
    "    update_unused = False\n",
    "    while index < len_bytes:\n",
    "        if index == len_bytes - 1:\n",
    "            encoded.append(input_bytes[index])\n",
    "            break\n",
    "        \n",
    "        packed_bp = input_bytes[index] | (input_bytes[index+1] << 32)\n",
    "\n",
    "        if packed_bp == top_bp:\n",
    "            encoded.append(unused)\n",
    "            dictionary[unused] = top_bp\n",
    "            index += 2\n",
    "            update_unused = True\n",
    "        else:\n",
    "            encoded.append(input_bytes[index])\n",
    "            index += 1\n",
    "    \n",
    "    if update_unused:\n",
    "        unused += 1\n",
    "    \n",
    "    return bp_encoding(encoded, dictionary, unused, level + 1, max_level)\n",
    "\n",
    "def bp_encode(input_str:str, max_level=20) -> [str, dict[int,int]]:\n",
    "    dictionary={}\n",
    "    input_bytes = [int(b) for b in input_str.encode(\"utf-8\")]\n",
    "\n",
    "    return bp_encoding(input_bytes, dictionary, max_level=max_level), dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab90c877-959f-4362-ada8-2cb4ca338663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_decode(input_tokens: list[int], dictionary: dict) -> list[int]:\n",
    "    decoded_tokens = list(input_tokens)\n",
    "\n",
    "    # Sort the dictionary keys (new symbols) in descending order.\n",
    "    # We must reverse the substitutions, so we start with the last encoded symbol.\n",
    "    symbols_to_expand = sorted(dictionary.keys(), reverse=True)\n",
    "\n",
    "    for symbol in symbols_to_expand:\n",
    "        # Get the packed pair integer for the current symbol\n",
    "        packed_pair = dictionary[symbol]\n",
    "        \n",
    "        # Token_2 (MSB): Shift right by 32\n",
    "        # Token_1 (LSB): Use bitwise AND with 0xFFFF to isolate the lower 32 bits\n",
    "        token_2 = packed_pair >> 32\n",
    "        token_1 = packed_pair & 0xFFFFFFFF\n",
    "        original_pair = [token_1, token_2] # Note: Order is LSB then MSB\n",
    "\n",
    "        new_decoded_tokens = []\n",
    "        i = 0\n",
    "        while i < len(decoded_tokens):\n",
    "            if decoded_tokens[i] == symbol:\n",
    "                # Replace the encoded symbol with its original two tokens\n",
    "                new_decoded_tokens.extend(original_pair)\n",
    "            else:\n",
    "                new_decoded_tokens.append(decoded_tokens[i])\n",
    "            i += 1\n",
    "        \n",
    "        decoded_tokens = new_decoded_tokens\n",
    "\n",
    "    return bytes(decoded_tokens).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dec4510a-8da0-4b9c-bfc1-16f59c80cdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ˜€ðŸ˜ƒðŸ˜„ðŸ˜ðŸ˜†ðŸ˜…ðŸ¤£ðŸ˜‚ðŸ™‚ðŸ™ƒ\n",
      "[257, 128, 257, 131, 257, 132, 257, 129, 257, 134, 257, 133, 256, 164, 163, 257, 259, 259, 131]\n",
      "{256: 682899800304, 257: 652835029248, 258: 1099511627906, 259: 657129996546}\n",
      "{682899800304: 256, 652835029248: 257, 1099511627906: 258, 657129996546: 259}\n",
      "ðŸ˜€ðŸ˜ƒðŸ˜„ðŸ˜ðŸ˜†ðŸ˜…ðŸ¤£ðŸ˜‚ðŸ™‚ðŸ™ƒ\n"
     ]
    }
   ],
   "source": [
    "input_str = \"ðŸ˜€ðŸ˜ƒðŸ˜„ðŸ˜ðŸ˜†ðŸ˜…ðŸ¤£ðŸ˜‚ðŸ™‚ðŸ™ƒ\"\n",
    "print(input_str)\n",
    "e, dictionary = bp_encode(input_str, max_level=25)\n",
    "rev_dictionary = {v:k for k,v in dictionary.items()}\n",
    "print(e)\n",
    "print(dictionary)\n",
    "print(rev_dictionary)\n",
    "print(bp_decode(e, dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c129f0-4e09-41de-8e9f-cfd2edfb3269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
